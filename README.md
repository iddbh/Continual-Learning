# Continual Learning
## 2025
* Muppidi, A., Zhang, Z. and Yang, H., 2025. [Fast trac: A parameter-free optimizer for lifelong reinforcement learning](https://proceedings.neurips.cc/paper_files/paper/2024/file/5b76d77e7095c6480ed827b85f0c2878-Paper-Conference.pdf). Advances in Neural Information Processing Systems, 37, pp.51169-51195.
## 2024
* Muppidi, A., Zhang, Z. and Yang, H., 2024. [Pick up the PACE: A Parameter-Free Optimizer for Lifelong Reinforcement Learning](https://arxiv.org/abs/2405.16642)
. arXiv preprint arXiv:2405.16642. [[website](https://computationalrobotics.seas.harvard.edu/TRAC/) + [python](https://github.com/ComputationalRobotics/TRAC?tab=readme-ov-file)]
* Julian, J., Koh, Y.S. and Bifet, A., 2024, August. [Sketch-Based Replay Projection for Continual Learning](https://dl.acm.org/doi/pdf/10.1145/3637528.3671714). In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (pp. 1325-1335). [[python](https://github.com/jjul482/Sketched-Replay-Projection)]
* Wang, L., Xie, J., Zhang, X., Huang, M., Su, H. and Zhu, J., 2024. [Hierarchical decomposition of prompt-based continual learning: Rethinking obscured sub-optimality](https://proceedings.neurips.cc/paper_files/paper/2023/file/d9f8b5abc8e0926539ecbb492af7b2f1-Paper-Conference.pdf). Advances in Neural Information Processing Systems, 36. [[python](https://github.com/thu-ml/HiDe-Prompt)]
* Hartvigsen, T., Sankaranarayanan, S., Palangi, H., Kim, Y. and Ghassemi, M., 2024. [Aging with grace: Lifelong model editing with discrete key-value adaptors](https://proceedings.neurips.cc/paper_files/paper/2023/file/95b6e2ff961580e03c0a662a63a71812-Paper-Conference.pdf). Advances in Neural Information Processing Systems, 36.
* Elsayed, M. and Mahmood, A.R., 2024. [Addressing loss of plasticity and catastrophic forgetting in continual learning](https://arxiv.org/pdf/2404.00781). [[python](https://github.com/mohmdelsayed/upgd)] arXiv preprint arXiv:2404.00781.
## 2023
* Abbas, Z., Zhao, R., Modayil, J., White, A. and Machado, M.C., 2023, November. [Loss of plasticity in continual deep reinforcement learning](https://www.nature.com/articles/s41586-024-07711-7). In Conference on Lifelong Learning Agents (pp. 620-636). PMLR. [[python](https://github.com/shibhansh/loss-of-plasticity)]
* Irie, K., Csord√°s, R. and Schmidhuber, J., 2023. [Automating Continual Learning](https://openreview.net/pdf?id=5twh6pM4SR).
* Mok, J., Do, J., Lee, S., Taghavi, T., Yu, S. and Yoon, S., 2023, July. [Large-scale lifelong learning of in-context instructions and how to tackle it](https://aclanthology.org/2023.acl-long.703.pdf). In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 12573-12589). [[website](https://aclanthology.org/2023.acl-long.703/)]
* Lu, P., Caprio, M., Eaton, E. and Lee, I., 2023. [IBCL: Zero-shot Model Generation for Task Trade-offs in Continual Learning](https://arxiv.org/pdf/2305.14782). arXiv preprint arXiv:2305.14782. [[python](https://github.com/ibcl-anon/ibcl)]
* Anand, N. and Precup, D., 2024. [Prediction and control in continual reinforcement learning](https://proceedings.neurips.cc/paper_files/paper/2023/file/c94bbbef466ab1b2cfa100e41413b3a8-Paper-Conference.pdf). Advances in Neural Information Processing Systems, 36.
* Abbas, Z., Zhao, R., Modayil, J., White, A. and Machado, M.C., 2023, November. [Loss of plasticity in continual deep reinforcement learning](https://proceedings.mlr.press/v232/abbas23a/abbas23a.pdf). In Conference on lifelong learning agents (pp. 620-636). PMLR.
* Lee, H., Cho, H., Kim, H., Gwak, D., Kim, J., Choo, J., Yun, S.Y. and Yun, C., 2023. [Plastic: Improving input and label plasticity for sample efficient reinforcement learning. Advances in Neural Information Processing Systems](https://proceedings.neurips.cc/paper_files/paper/2023/file/c464fc4516aca4e68f2a14e67c6f0402-Paper-Conference.pdf),[[python](https://github.com/dojeon-ai/plastic)] 36, pp.62270-62295.
* Lyle, C., Zheng, Z., Nikishin, E., Pires, B.A., Pascanu, R. and Dabney, W., 2023, July. [Understanding plasticity in neural networks](https://proceedings.mlr.press/v202/lyle23b/lyle23b.pdf). In International Conference on Machine Learning (pp. 23190-23211). PMLR.
* Nikishin, E., Oh, J., Ostrovski, G., Lyle, C., Pascanu, R., Dabney, W. and Barreto, A., 2023. [Deep reinforcement learning with plasticity injection](https://proceedings.neurips.cc/paper_files/paper/2023/file/75101364dc3aa7772d27528ea504472b-Paper-Conference.pdf). Advances in Neural Information Processing Systems, 36, pp.37142-37159.
## 2022
* Savage, N., 2022. [Learning over a lifetime](https://www.nature.com/articles/d41586-022-01962-y). Nature.
* Zhang, Y., Pfahringer, B., Frank, E., Bifet, A., Lim, N.J.S. and Jia, Y., 2022, November. [Repeated augmented rehearsal: a simple but strong baseline for online continual learning](https://papers.neurips.cc/paper_files/paper/2022/file/5ebbbac62b968254093023f1c95015d3-Paper-Conference.pdf). In Proceedings of the 36th International Conference on Neural Information Processing Systems (pp. 14771-14783).[[python](https://github.com/YaqianZhang/RepeatedAugmentedRehearsal/tree/master)]
* Gaya, J.B., Doan, T., Caccia, L., Soulier, L., Denoyer, L. and Raileanu, R., 2022. [Building a subspace of policies for scalable continual learning](https://arxiv.org/pdf/2211.10445). arXiv preprint arXiv:2211.10445.
* Elsayed, M. and Mahmood, A.R., 2022. [Hesscale: Scalable computation of hessian diagonals](https://arxiv.org/pdf/2210.11639). arXiv preprint arXiv:2210.11639.
## 2021
* Dohare, S., Sutton, R.S. and Mahmood, A.R., 2021. [Continual backprop: Stochastic gradient descent with persistent randomness](https://arxiv.org/abs/2108.06325). arXiv preprint arXiv:2108.06325. [[python](https://github.com/shibhansh/loss-of-plasticity)]
* Gaya, J.B., Soulier, L. and Denoyer, L., 2021. [Learning a subspace of policies for online adaptation in reinforcement learning](https://arxiv.org/pdf/2110.05169). arXiv preprint arXiv:2110.05169.
## 2020
* Gupta, G., Yadav, K. and Paull, L., 2020. [Look-ahead meta learning for continual learning](https://proceedings.neurips.cc/paper_files/paper/2020/file/85b9a5ac91cd629bd3afe396ec07270a-Paper.pdf). Advances in Neural Information Processing Systems, 33, pp.11588-11598.[[python](https://github.com/montrealrobotics/La-MAML?tab=readme-ov-file)]
* Ash, J. and Adams, R.P., 2020. [On warm-starting neural network training](https://proceedings.neurips.cc/paper/2020/file/288cd2567953f06e460a33951f55daaf-Paper.pdf). Advances in neural information processing systems, 33, pp.3884-3894.
## 2019
* Cutkosky, A., 2019, June. [Combining online learning guarantees](https://proceedings.mlr.press/v99/cutkosky19b/cutkosky19b.pdf). In Conference on Learning Theory (pp. 895-913). PMLR.
## 2018
* Cutkosky, A. and Orabona, F., 2018, July. [Black-box reductions for parameter-free online learning in banach spaces](https://proceedings.mlr.press/v75/cutkosky18a/cutkosky18a.pdf). In Conference On Learning Theory (pp. 1493-1529). PMLR.
