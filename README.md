# Continual Learning
## 2025
* Muppidi, A., Zhang, Z. and Yang, H., 2025. [Fast trac: A parameter-free optimizer for lifelong reinforcement learning](https://proceedings.neurips.cc/paper_files/paper/2024/file/5b76d77e7095c6480ed827b85f0c2878-Paper-Conference.pdf). Advances in Neural Information Processing Systems, 37, pp.51169-51195.
* Feng, T., Li, W., Zhu, D., Yuan, H., Zheng, W., Zhang, D. and Tang, J., 2025. [ZeroFlow: Overcoming Catastrophic Forgetting is Easier than You Think](https://arxiv.org/pdf/2501.01045). arXiv preprint arXiv:2501.01045.
* Yichen Wu, Hongming Piao, Long-Kai Huang, Renzhen Wang, Wanhua Li, Hanspeter Pfister, Deyu Meng, Kede Ma, and Ying Wei. [Sd-lora: Scalable decoupled low-rank adaptation for class incremental learning](https://arxiv.org/pdf/2501.13198). ICLR, 2025.[[code](https://github.com/WuYichen-97/SD-Lora-CL)]
* Liao, H., He, S., Hao, Y., Zhao, J. and Liu, K., 2025. [DATA: Decomposed Attention-based Task Adaptation for Rehearsal-Free Continual Learning](https://arxiv.org/pdf/2502.11482?). arXiv preprint arXiv:2502.11482.[[code](https://github.com/Xnhyacinth/DATA)]
* He, J., Duan, Z. and Zhu, F., 2025. [CL-LoRA: Continual Low-Rank Adaptation for Rehearsal-Free Class-Incremental Learning](https://openaccess.thecvf.com/content/CVPR2025/papers/He_CL-LoRA_Continual_Low-Rank_Adaptation_for_Rehearsal-Free_Class-Incremental_Learning_CVPR_2025_paper.pdf). In Proceedings of the Computer Vision and Pattern Recognition Conference (pp. 30534-30544).[[code](https://github.com/JiangpengHe/CL-LoRA)]

## 2024

* Muppidi, A., Zhang, Z. and Yang, H., 2024. [Pick up the PACE: A Parameter-Free Optimizer for Lifelong Reinforcement Learning](https://arxiv.org/abs/2405.16642)
. arXiv preprint arXiv:2405.16642. [[website](https://computationalrobotics.seas.harvard.edu/TRAC/) + [python](https://github.com/ComputationalRobotics/TRAC?tab=readme-ov-file)]
* Julian, J., Koh, Y.S. and Bifet, A., 2024, August. [Sketch-Based Replay Projection for Continual Learning](https://dl.acm.org/doi/pdf/10.1145/3637528.3671714). In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (pp. 1325-1335). [[python](https://github.com/jjul482/Sketched-Replay-Projection)]
* Wang, L., Xie, J., Zhang, X., Huang, M., Su, H. and Zhu, J., 2024. [Hierarchical decomposition of prompt-based continual learning: Rethinking obscured sub-optimality](https://proceedings.neurips.cc/paper_files/paper/2023/file/d9f8b5abc8e0926539ecbb492af7b2f1-Paper-Conference.pdf). Advances in Neural Information Processing Systems, 36. [[python](https://github.com/thu-ml/HiDe-Prompt)]
* Hartvigsen, T., Sankaranarayanan, S., Palangi, H., Kim, Y. and Ghassemi, M., 2024. [Aging with grace: Lifelong model editing with discrete key-value adaptors](https://proceedings.neurips.cc/paper_files/paper/2023/file/95b6e2ff961580e03c0a662a63a71812-Paper-Conference.pdf). Advances in Neural Information Processing Systems, 36.
* Elsayed, M. and Mahmood, A.R., 2024.
  [Addressing loss of plasticity and catastrophic forgetting in continual learning]().
  In International Conference on Learning Representations.
  * [[Python]](https://github.com/mohmdelsayed/upgd)
* Wang, Zhenyi, Enneng Yang, Li Shen, and Heng Huang. [A comprehensive survey of forgetting in deep learning beyond continual learning](https://arxiv.org/pdf/2307.09218). [[github](https://github.com/EnnengYang/Awesome-Forgetting-in-Deep-Learning)] IEEE Transactions on Pattern Analysis and Machine Intelligence (2024).
* Bellitto, G., Proietto Salanitri, F., Pennisi, M., Boschini, M., Bonicelli, L., Porrello, A., Calderara, S., Palazzo, S. and Spampinato, C., 2024. [Saliency-driven experience replay for continual learning](https://proceedings.neurips.cc/paper_files/paper/2024/file/bb1e9f32181a8d6a834670d5b3e1c48d-Paper-Conference.pdf). Advances in Neural Information Processing Systems, 37, pp.103356-103383.[[code](https://github.com/perceivelab/SER)]
* Zhuang, H., Chen, Y., Fang, D., He, R., Tong, K., Wei, H., Zeng, Z. and Chen, C., 2024. [GACL: Exemplar-free generalized analytic continual learning](https://arxiv.org/abs/2403.15706). Advances in Neural Information Processing Systems, 37, pp.83024-83047.[[code](https://github.com/CHEN-YIZHU/GACL)]
* Zhou, D.W., Cai, Z.W., Ye, H.J., Zhan, D.C. and Liu, Z., 2025. [Revisiting class-incremental learning with pre-trained models: Generalizability and adaptivity are all you need](https://arxiv.org/pdf/2303.07338). International Journal of Computer Vision, 133(3), pp.1012-1032.[[code](https://github.com/LAMDA-CL/RevisitingCIL)]
* Liang, Y.S. and Li, W.J., 2024. [Inflora: Interference-free low-rank adaptation for continual learning](https://openaccess.thecvf.com/content/CVPR2024/papers/Liang_InfLoRA_Interference-Free_Low-Rank_Adaptation_for_Continual_Learning_CVPR_2024_paper.pdf). In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 23638-23647).[[code](https://github.com/liangyanshuo/InfLoRA)]


## 2023
* Abbas, Z., Zhao, R., Modayil, J., White, A. and Machado, M.C., 2023, November. [Loss of plasticity in continual deep reinforcement learning](https://www.nature.com/articles/s41586-024-07711-7). In Conference on Lifelong Learning Agents (pp. 620-636). PMLR. [[python](https://github.com/shibhansh/loss-of-plasticity)]
* Irie, K., Csordás, R. and Schmidhuber, J., 2023. [Automating Continual Learning](https://openreview.net/pdf?id=5twh6pM4SR).
* Mok, J., Do, J., Lee, S., Taghavi, T., Yu, S. and Yoon, S., 2023, July. [Large-scale lifelong learning of in-context instructions and how to tackle it](https://aclanthology.org/2023.acl-long.703.pdf). In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 12573-12589). [[website](https://aclanthology.org/2023.acl-long.703/)]
* Lu, P., Caprio, M., Eaton, E. and Lee, I., 2023. [IBCL: Zero-shot Model Generation for Task Trade-offs in Continual Learning](https://arxiv.org/pdf/2305.14782). arXiv preprint arXiv:2305.14782. [[python](https://github.com/ibcl-anon/ibcl)]
* Anand, N. and Precup, D., 2024. [Prediction and control in continual reinforcement learning](https://proceedings.neurips.cc/paper_files/paper/2023/file/c94bbbef466ab1b2cfa100e41413b3a8-Paper-Conference.pdf). Advances in Neural Information Processing Systems, 36.
* Abbas, Z., Zhao, R., Modayil, J., White, A. and Machado, M.C., 2023, November. [Loss of plasticity in continual deep reinforcement learning](https://proceedings.mlr.press/v232/abbas23a/abbas23a.pdf). In Conference on lifelong learning agents (pp. 620-636). PMLR.
* Lee, H., Cho, H., Kim, H., Gwak, D., Kim, J., Choo, J., Yun, S.Y. and Yun, C., 2023. [Plastic: Improving input and label plasticity for sample efficient reinforcement learning. Advances in Neural Information Processing Systems](https://proceedings.neurips.cc/paper_files/paper/2023/file/c464fc4516aca4e68f2a14e67c6f0402-Paper-Conference.pdf),[[python](https://github.com/dojeon-ai/plastic)] 36, pp.62270-62295.
* Lyle, C., Zheng, Z., Nikishin, E., Pires, B.A., Pascanu, R. and Dabney, W., 2023, July. [Understanding plasticity in neural networks](https://proceedings.mlr.press/v202/lyle23b/lyle23b.pdf). In International Conference on Machine Learning (pp. 23190-23211). PMLR.
* Nikishin, E., Oh, J., Ostrovski, G., Lyle, C., Pascanu, R., Dabney, W. and Barreto, A., 2023. [Deep reinforcement learning with plasticity injection](https://proceedings.neurips.cc/paper_files/paper/2023/file/75101364dc3aa7772d27528ea504472b-Paper-Conference.pdf). Advances in Neural Information Processing Systems, 36, pp.37142-37159.
* Cha, H., Lee, J. and Shin, J., 2021. Co2l: [Contrastive continual learning](https://openaccess.thecvf.com/content/ICCV2021/html/Cha_Co2L_Contrastive_Continual_Learning_ICCV_2021_paper.html). In Proceedings of the IEEE/CVF International conference on computer vision (pp. 9516-9525).
* Sun, Z., Mu, Y. and Hua, G., 2023. [Regularizing second-order influences for continual learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Regularizing_Second-Order_Influences_for_Continual_Learning_CVPR_2023_paper.pdf). In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 20166-20175).[[code](https://github.com/feifeiobama/InfluenceCL)]
* Liang, Y.S. and Li, W.J., 2023. [Adaptive plasticity improvement for continual learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Liang_Adaptive_Plasticity_Improvement_for_Continual_Learning_CVPR_2023_paper.pdf). In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 7816-7825).
## 2022
* Savage, N., 2022. [Learning over a lifetime](https://www.nature.com/articles/d41586-022-01962-y). Nature.
* Zhang, Y., Pfahringer, B., Frank, E., Bifet, A., Lim, N.J.S. and Jia, Y., 2022, November. [Repeated augmented rehearsal: a simple but strong baseline for online continual learning](https://papers.neurips.cc/paper_files/paper/2022/file/5ebbbac62b968254093023f1c95015d3-Paper-Conference.pdf). In Proceedings of the 36th International Conference on Neural Information Processing Systems (pp. 14771-14783).[[python](https://github.com/YaqianZhang/RepeatedAugmentedRehearsal/tree/master)]
* Gaya, J.B., Doan, T., Caccia, L., Soulier, L., Denoyer, L. and Raileanu, R., 2022. [Building a subspace of policies for scalable continual learning](https://arxiv.org/pdf/2211.10445). arXiv preprint arXiv:2211.10445.
* Elsayed, M. and Mahmood, A.R., 2022. [Hesscale: Scalable computation of hessian diagonals](https://arxiv.org/pdf/2210.11639). arXiv preprint arXiv:2210.11639.
* Wang, Z., Zhang, Z., Lee, C.Y., Zhang, H., Sun, R., Ren, X., Su, G., Perot, V., Dy, J. and Pfister, T., 2022. [Learning to prompt for continual learning](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Learning_To_Prompt_for_Continual_Learning_CVPR_2022_paper.pdf). In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 139-149).

## 2021
* Dohare, S., Sutton, R.S. and Mahmood, A.R., 2021. [Continual backprop: Stochastic gradient descent with persistent randomness](https://arxiv.org/abs/2108.06325). arXiv preprint arXiv:2108.06325. [[python](https://github.com/shibhansh/loss-of-plasticity)]
* Gaya, J.B., Soulier, L. and Denoyer, L., 2021. [Learning a subspace of policies for online adaptation in reinforcement learning](https://arxiv.org/pdf/2110.05169). arXiv preprint arXiv:2110.05169.
* Ebrahimi, S., Petryk, S., Gokul, A., Gan, W., Gonzalez, J.E., Rohrbach, M. and Darrell, T., 2021. [Remembering for the right reasons: Explanations reduce catastrophic forgetting](https://openreview.net/pdf?id=tHgJoMfy6nI). Applied AI letters, 2(4), p.e44.[[code](https://github.com/SaynaEbrahimi/Remembering-for-the-Right-Reasons)]
* Caccia, L., Aljundi, R., Asadi, N., Tuytelaars, T., Pineau, J. and Belilovsky, E., [New Insights on Reducing Abrupt Representation Change in Online Continual Learning](https://arxiv.org/pdf/2104.05025). In International Conference on Learning Representations.[[code](https://github.com/pclucas14/AML)]
## 2020
* Gupta, G., Yadav, K. and Paull, L., 2020. [Look-ahead meta learning for continual learning](https://proceedings.neurips.cc/paper_files/paper/2020/file/85b9a5ac91cd629bd3afe396ec07270a-Paper.pdf). Advances in Neural Information Processing Systems, 33, pp.11588-11598.[[python](https://github.com/montrealrobotics/La-MAML?tab=readme-ov-file)]
* Ash, J. and Adams, R.P., 2020. [On warm-starting neural network training](https://proceedings.neurips.cc/paper/2020/file/288cd2567953f06e460a33951f55daaf-Paper.pdf). Advances in neural information processing systems, 33, pp.3884-3894.
* Harrison, J., Sharma, A., Finn, C. and Pavone, M., 2020. [Continuous meta-learning without tasks](https://proceedings.neurips.cc/paper/2020/file/cc3f5463bc4d26bc38eadc8bcffbc654-Paper.pdf). Advances in neural information processing systems, 33, pp.17571-17581.[[code](https://github.com/StanfordASL/moca)]
* Buzzega, P., Boschini, M., Porrello, A., Abati, D. and Calderara, S., 2020. [Dark experience for general continual learning: a strong, simple baseline](https://proceedings.neurips.cc/paper/2020/file/b704ea2c39778f07c617f6b7ce480e9e-Paper.pdf). Advances in neural information processing systems, 33, pp.15920-15930.[[code](https://github.com/aimagelab/mammoth)]
## 2019
* Cutkosky, A., 2019, June. [Combining online learning guarantees](https://proceedings.mlr.press/v99/cutkosky19b/cutkosky19b.pdf). In Conference on Learning Theory (pp. 895-913). PMLR.
* Aljundi, R., Lin, M., Goujaud, B. and Bengio, Y., 2019. [Gradient based sample selection for online continual learning](https://proceedings.neurips.cc/paper_files/paper/2019/file/e562cd9c0768d5464b64cf61da7fc6bb-Paper.pdf). Advances in neural information processing systems, 32.[[code](https://github.com/rahafaljundi/Gradient-based-Sample-Selection)]
## 2018
* Cutkosky, A. and Orabona, F., 2018, July. [Black-box reductions for parameter-free online learning in banach spaces](https://proceedings.mlr.press/v75/cutkosky18a/cutkosky18a.pdf). In Conference On Learning Theory (pp. 1493-1529). PMLR.

## 1950s

* SELFRIDGE, O., 1958.
  [Pandemonium - A paradigm for learning](http://www.incompleteideas.net/papers/pandemonium2.pdf).
  In Symposium on the Mechanization of Thought Processes. National Physical Laboratory, UK.
